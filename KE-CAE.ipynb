{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c5872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image, ImageEnhance\n",
    "import numbers\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import autograd\n",
    "\n",
    "img_shape = (1, 128, 128)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "def weights_init_uniform(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        m.bias.data.fill_(0.1)\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        m.bias.data.fill_(0.1)\n",
    "\n",
    "\n",
    "# block 32 64 128 256 represent how many channel for conv3 in this block \n",
    "# and the image dimension after convolution is presented\n",
    "class Block32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            # input --> 1 * 128 * 128\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),  # 32 * 126 * 126\n",
    "            # normalization\n",
    "            nn.BatchNorm2d(32, eps=1e-5),\n",
    "            #  activate function\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),  # 32 * 124 * 124\n",
    "            nn.BatchNorm2d(32, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.block(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Block64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),  # 64 * 61 * 61\n",
    "            nn.BatchNorm2d(64, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),  # 64 * 59 * 59\n",
    "            nn.BatchNorm2d(64, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),  # 64 * 57 * 57\n",
    "            nn.BatchNorm2d(64, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.block(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Block128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),  # 128 * 28 * 28\n",
    "            nn.BatchNorm2d(128, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0),  # 128 * 26 * 26\n",
    "            nn.BatchNorm2d(128, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0),  # 128 * 24 * 24\n",
    "            nn.BatchNorm2d(128, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0),  # 128 * 22 * 22\n",
    "            nn.BatchNorm2d(128, eps=1e-5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.block(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Block256(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0),  # 256 * 10 * 10\n",
    "            nn.BatchNorm2d(256, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0),  # 256 * 8 * 8\n",
    "            nn.BatchNorm2d(256, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0),  # 256 * 6 * 6\n",
    "            nn.BatchNorm2d(256, eps=1e-5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0),  # 256 * 4 * 4\n",
    "            nn.BatchNorm2d(256, eps=1e-5),\n",
    "            nn.ReLU(inplace=True)\n",
    "            # nn.Conv2d(256, 256, kernel_size=4, stride=1, padding=0),  # 256 * 1 * 1\n",
    "            # nn.BatchNorm2d(256, eps=1e-5),\n",
    "            # nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.block(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# channelattention\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, kernel):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(kernel, kernel // 8, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(kernel // 8, kernel, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return x * self.sigmoid(out)\n",
    "\n",
    "\n",
    "# spatialattention(I did try to combine CA with SA but the result was not very good as espected)\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        y = torch.cat([avg_out, max_out], dim=1)\n",
    "        y = self.conv1(y)\n",
    "        return x * self.sigmoid(y)\n",
    "\n",
    "\n",
    "class Siamese_VGG(nn.Module):\n",
    "    def __init__(self, drop, use_w_init=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # blocks\n",
    "        self.block32 = Block32()\n",
    "        self.block64 = Block64()\n",
    "        self.block128 = Block128()\n",
    "        self.block256 = Block256()\n",
    "\n",
    "        # only one fully connecter was used\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Dropout(p=drop),\n",
    "            # combine all features to 2 dimention because of binaire classfication\n",
    "            nn.Linear(32 + 64 + 128 + 256, 2)\n",
    "        )\n",
    "\n",
    "        # initialization of the weights and bias \n",
    "        if use_w_init:\n",
    "            self.apply(weights_init_uniform)\n",
    "\n",
    "        # channel attention\n",
    "        self.attention32 = ChannelAttention(32)\n",
    "        self.attention64 = ChannelAttention(64)\n",
    "        self.attention128 = ChannelAttention(128)\n",
    "        self.attention256 = ChannelAttention(256)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # forward propagation\n",
    "        o1 = self.block32(x1)\n",
    "        o2 = self.block32(x2)\n",
    "        att_o1 = self.attention32(o1)\n",
    "        att_o2 = self.attention32(o2)\n",
    "        # use GAP to extract the features after the block\n",
    "        feats1_1 = torch.nn.functional.adaptive_avg_pool2d(o1, (1, 1))\n",
    "        feats1_2 = torch.nn.functional.adaptive_avg_pool2d(o2, (1, 1))\n",
    "        # use add bit-by-bit to combine 2 GAP\n",
    "        feats1 = feats1_1.view((feats1_1).size(0), -1) + feats1_2.view((feats1_2).size(0), -1)\n",
    "\n",
    "        # now input = output from last block + channel attention\n",
    "        o1 = self.block64(o1 + att_o1)\n",
    "        o2 = self.block64(o2 + att_o2)\n",
    "        att_o1 = self.attention64(o1)\n",
    "        att_o2 = self.attention64(o2)\n",
    "        feats2_1 = torch.nn.functional.adaptive_avg_pool2d(o1, (1, 1))\n",
    "        feats2_2 = torch.nn.functional.adaptive_avg_pool2d(o2, (1, 1))\n",
    "        feats2 = feats2_1.view((feats2_1).size(0), -1) + feats2_2.view((feats2_2).size(0), -1)\n",
    "\n",
    "        o1 = self.block128(o1 + att_o1)\n",
    "        o2 = self.block128(o2 + att_o2)\n",
    "        att_o1 = self.attention128(o1)\n",
    "        att_o2 = self.attention128(o2)\n",
    "        feats3_1 = torch.nn.functional.adaptive_avg_pool2d(o1, (1, 1))\n",
    "        feats3_2 = torch.nn.functional.adaptive_avg_pool2d(o2, (1, 1))\n",
    "        feats3 = feats3_1.view((feats3_1).size(0), -1) + feats3_2.view((feats3_2).size(0), -1)\n",
    "\n",
    "        o1 = self.block256(o1 + att_o1)\n",
    "        o2 = self.block256(o2 + att_o2)\n",
    "        # att_o1 = self.attention256(o1)\n",
    "        # att_o2 = self.attention256(o2)\n",
    "        feats4_1 = torch.nn.functional.adaptive_avg_pool2d(o1, (1, 1))\n",
    "        feats4_2 = torch.nn.functional.adaptive_avg_pool2d(o2, (1, 1))\n",
    "        feats4 = feats4_1.view((feats4_1).size(0), -1) + feats4_2.view((feats4_2).size(0), -1)  #\n",
    "\n",
    "        # combine all GAPs to get the final features\n",
    "        feats_final = torch.cat([feats1, feats2], 1)\n",
    "        feats_final = torch.cat([feats_final, feats3], 1)\n",
    "        feats_final = torch.cat([feats_final, feats4], 1)\n",
    "\n",
    "        return self.final(feats_final), feats_final\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # k+(in-1)s-2p\n",
    "            nn.ConvTranspose2d(2048, 128*32, 4, 1, 0, bias=False), # 4096, 4, 4\n",
    "            nn.BatchNorm2d(128 * 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128*32, 128 * 16, 4, 2, 1, bias=False), # 2048, 8, 8 # 8\n",
    "            nn.BatchNorm2d(128 * 16),\n",
    "            nn.ReLU(True),\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(128 * 16, 128 * 8, 4, 2, 0, bias=False), # 1024, 20, 20 # 18\n",
    "            nn.BatchNorm2d(128 * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(128 * 8, 128 * 4, 4, 2, 1, bias=False), # 512, 40, 40 # 36\n",
    "            nn.BatchNorm2d(128 * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(128 * 4, 128 * 2, 4, 2, 0, bias=False), # 256, 80, 80 # 74\n",
    "            nn.BatchNorm2d(128 * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(128 * 2, 128, 4, 2, 1, bias=False), # 128, 160, 160 # 128, # 148\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(128, 1, 7, 2, 1, bias=False), # 1, 320, 320   # 1,299,299 # 299\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # print(z.shape)\n",
    "        img = self.main(z)\n",
    "        # print(img.size())\n",
    "        # img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            # input is 1 x 128 x 128 1*299*299\n",
    "            nn.Conv2d(1, 32, 3, 2, 0, bias=False),  # 32*63*63 32*149*149\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(32, 32 * 2, 3, 2, 0, bias=False),  # 64*31*31 64*74*74\n",
    "            nn.BatchNorm2d(32 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(32 * 2, 32 * 4, 3, 2, 0, bias=False),  # 128*15*15 128*36*36\n",
    "            nn.BatchNorm2d(32 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(32 * 4, 32 * 8, 3, 2, 0, bias=False),  # 256*7*7 256*17*17\n",
    "            nn.BatchNorm2d(32 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(32 * 8, 32 * 16, 3, 2, 0, bias=False),  # 512*3*3 512 * 8 * 8\n",
    "            nn.BatchNorm2d(32 * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32 * 16, 32 * 32, 3, 2, 0, bias=False),  # 1024*1*1 1024 * 3* 3\n",
    "            nn.BatchNorm2d(32 * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32 * 32, 32 * 64, 3, 2, 0, bias=False),  # 2048*1*1 2048 * 1* 1\n",
    "            nn.BatchNorm2d(32 * 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            # input is 1 x 128 x 128\n",
    "            nn.Conv2d(1, 32, 3, 2, 0, bias=False),  # 32*63*63\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(32, 32 * 2, 3, 2, 0, bias=False),  # 64*31*31\n",
    "            nn.BatchNorm2d(32 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(32 * 2, 32 * 4, 3, 2, 0, bias=False),  # 128*15*15\n",
    "            nn.BatchNorm2d(32 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(32 * 4, 32 * 8, 3, 2, 0, bias=False),  # 256*7*7\n",
    "            nn.BatchNorm2d(32 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(32 * 8, 32 * 16, 3, 2, 0, bias=False),  # 512*3*3\n",
    "            nn.BatchNorm2d(32 * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32 * 16, 32 * 32, 3, 2, 0, bias=False),  # 1024*1*1\n",
    "            nn.BatchNorm2d(32 * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32 * 32, 32 * 64, 3, 2, 0, bias=False),  # 1024*1*1\n",
    "            nn.BatchNorm2d(32 * 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        # print(img.size())\n",
    "        output1 = self.encoder1(img)\n",
    "        output2 = self.encoder2(img)\n",
    "        return output1,output2\n",
    "    \n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def get_pair(I):\n",
    "    s = I.size[0]\n",
    "    pad = int(np.floor(s / 3))\n",
    "    ps = 128\n",
    "    l = I.crop([0, pad, ps, pad + ps])\n",
    "    m = I.crop([s - ps, pad, s, pad + ps])\n",
    "    m = m.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    return l, m\n",
    "\n",
    "\n",
    "class KneeGradingDataset(data.Dataset):\n",
    "    def __init__(self, dataset, transform, augment, stage='train'):\n",
    "        super(KneeGradingDataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        # self.train_files = train_files\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.stage = stage\n",
    "        if self.stage == 'train':\n",
    "            self.images, self.labels = self.load_csv(\"paper_0_2_100000.csv\")\n",
    "        if self.stage == 'valid':\n",
    "            self.images, self.labels = self.load_csv(\"trainData_0_2.csv\")\n",
    "\n",
    "    def load_csv(self, filename):\n",
    "        images, labels = [], []\n",
    "        with open(os.path.join(self.dataset, filename)) as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                img_0, l_0, img_2, l_2 = row\n",
    "                # print(img_0, img_2)\n",
    "                label_0 = int(l_0)\n",
    "                label_2 = int(l_2)       \n",
    "                images.append([img_0, img_2])\n",
    "                labels.append([label_0,label_2])\n",
    "                #print(labels)\n",
    "                #break\n",
    "        return images, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_0, img_2, label_0, label_2 = self.images[index][0], self.images[index][1], self.labels[index][0],self.labels[index][1]\n",
    "        fname = os.path.join(self.dataset, img_0[0], img_0)\n",
    "        fname2 = os.path.join(self.dataset, img_2[0], img_2)\n",
    "        img = Image.open(fname)\n",
    "        img2 = Image.open(fname2)\n",
    "        img = self.augment(img)\n",
    "        img2 = self.augment(img2)\n",
    "        \n",
    "        l, m = get_pair(img)\n",
    "        l2, m2 = get_pair(img2)\n",
    "        \n",
    "        l = self.transform(l)\n",
    "        m = self.transform(m)\n",
    "        \n",
    "        l2 = self.transform(l2)\n",
    "        m2 = self.transform(m2)\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        img2 = self.transform(img2)\n",
    "        # print(label_0)\n",
    "        return img, img2, l, m, l2, m2, label_0, label_2\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.train_files)\n",
    "        return 100000\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        tw, th, = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "\n",
    "class distance_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(distance_loss, self).__init__()\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "        mean_no_related = torch.mean(x1)\n",
    "        mean_key = torch.mean(x2)\n",
    "        var_no_related = torch.var(x1)\n",
    "        var_key = torch.var(x2)\n",
    "        \n",
    "        loss = (var_no_related + var_key) / torch.square(mean_no_related - mean_key)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "def get_pair_Tensor(I):\n",
    "    tensor_l = torch.flip(I[:,:,100:228,171:299][0].cpu(), [-1]).unsqueeze(1)\n",
    "    tensor_m = I[:,:,100:228,0:128][0].unsqueeze(1)\n",
    "    for i in range(I.shape[0]-1):\n",
    "        tensor_l = torch.cat((tensor_l, torch.flip(I[:,:,100:228,171:299][i+1].cpu(), [-1]).unsqueeze(1)), 0)\n",
    "        tensor_m = torch.cat((tensor_m, I[:,:,100:228,0:128][i+1].unsqueeze(1)), 0) \n",
    "\n",
    "    return tensor_l,tensor_m\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "# discriminator = Discriminator()\n",
    "discriminator2 = Siamese_VGG(0.2)\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "CUDA = 0\n",
    "\n",
    "if cuda:\n",
    "    #discriminator.cuda(CUDA)\n",
    "    #discriminator.apply(weights_init)\n",
    "    discriminator2.cuda(CUDA)\n",
    "    discriminator2.apply(weights_init)\n",
    "    encoder.cuda(CUDA)\n",
    "    encoder.apply(weights_init)\n",
    "    decoder.cuda(CUDA)\n",
    "    decoder.apply(weights_init)\n",
    "\n",
    "# Configure data loader\n",
    "train_cats_length = []\n",
    "val_length = []\n",
    "test_length = []\n",
    "\n",
    "transf_tens = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "augment_transforms = transforms.Compose([\n",
    "    CenterCrop(299)\n",
    "])\n",
    "\n",
    "train_ds = KneeGradingDataset('./OAI_m', transform=transf_tens, augment=augment_transforms, stage='train')\n",
    "train_loader = data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "#optimizer_D = torch.optim.Adam(discriminator.parameters(),lr=0.0002)\n",
    "optimizer_D_ = torch.optim.Adam(discriminator2.parameters(),lr=0.0001)\n",
    "optimizer_E = torch.optim.Adam(encoder.parameters(),lr=0.0001)\n",
    "optimizer_DE = torch.optim.Adam(decoder.parameters(),lr=0.0001)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "batches_done = 0\n",
    "device = torch.device(\"cuda:\" + str(CUDA) if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "loss_Ds = []\n",
    "loss_Gs = []\n",
    "mse = nn.MSELoss()\n",
    "loss_Dis = distance_loss()\n",
    "\n",
    "loss_Ds = []\n",
    "loss_Rs = []\n",
    "loss_Distances = []\n",
    "loss_CEs = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (img, img2, l, m, l2, m2, label_0, label_2) in enumerate(train_loader):\n",
    "        # print(label_0)\n",
    "        img0 = Variable(img.cuda(CUDA))\n",
    "        img2 = Variable(img2.cuda(CUDA))\n",
    "        l0 = Variable(l.cuda(CUDA))\n",
    "        l2 = Variable(l2.cuda(CUDA))\n",
    "        m0 = Variable(m.cuda(CUDA))\n",
    "        m2 = Variable(m2.cuda(CUDA))\n",
    "        label_0 = Variable(label_0.long().cuda(CUDA))\n",
    "        label_2 = Variable(label_2.long().cuda(CUDA))\n",
    "        # real_imgs = torch.cat((real_imgs_l,real_imgs_r),dim=0)    \n",
    "        \n",
    "        vector_Key0 = encoder(img0)[0]   \n",
    "        vector_Unrelated0 = encoder(img0)[1]        \n",
    "        vector_img0 = vector_Key0 + vector_Unrelated0\n",
    "        \n",
    "        vector_Key2 = encoder(img2)[0]\n",
    "        vector_Unrelated2 = encoder(img2)[1]        \n",
    "        vector_img2 = vector_Key2 + vector_Unrelated2\n",
    "        \n",
    "        vector_Key_Exchange_img0 = vector_Key2 + vector_Unrelated0\n",
    "        vector_Key_Exchange_img2 = vector_Key0 + vector_Unrelated2\n",
    "        \n",
    "        R_img0 = decoder(vector_img0)\n",
    "        R_img2 = decoder(vector_img2)\n",
    "        \n",
    "        Key_Exchange_img0 = decoder(vector_Key_Exchange_img0)\n",
    "        Key_Exchange_img2 = decoder(vector_Key_Exchange_img2)\n",
    "        \n",
    "        # print(Key_Exchange_img0.size())\n",
    "        Key_Exchange_img0_l = get_pair_Tensor(Key_Exchange_img0)[0].cuda(CUDA)\n",
    "        Key_Exchange_img0_m = get_pair_Tensor(Key_Exchange_img0)[1].cuda(CUDA)\n",
    "        \n",
    "        Key_Exchange_img2_l = get_pair_Tensor(Key_Exchange_img2)[0].cuda(CUDA)\n",
    "        Key_Exchange_img2_m = get_pair_Tensor(Key_Exchange_img2)[1].cuda(CUDA)\n",
    "        \n",
    "        optimizer_D_.zero_grad()\n",
    "        \n",
    "        for p in discriminator2.parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        loss_D1 = F.cross_entropy(discriminator2(l0,m0)[0],label_0)\n",
    "        loss_D2 = F.cross_entropy(discriminator2(l2,m2)[0],label_2)\n",
    "        loss_D = loss_D1 + loss_D2\n",
    "        loss_Ds.append(loss_D)\n",
    "        loss_D.backward()\n",
    "        optimizer_D_.step()\n",
    "        \n",
    "        #loss_Ds.append(loss_D)\n",
    "        \n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_DE.zero_grad()\n",
    "        \n",
    "        for p in discriminator2.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        loss_distance0 = loss_Dis(vector_Key0, vector_Unrelated0)\n",
    "        loss_distance2 = loss_Dis(vector_Key2, vector_Unrelated2)\n",
    "        \n",
    "        loss_distance = loss_distance0 + loss_distance2\n",
    "        loss_Distances.append(loss_distance)\n",
    "        \n",
    "        loss_R0 = mse(img0, R_img0)\n",
    "        loss_R2 = mse(img2, R_img2)\n",
    "        \n",
    "        loss_R = loss_R0 + loss_R2\n",
    "        loss_Rs.append(loss_R)\n",
    "        \n",
    "        loss_ce0 = F.cross_entropy(discriminator2(Key_Exchange_img0_l, Key_Exchange_img0_m)[0],label_2)\n",
    "        loss_ce2 = F.cross_entropy(discriminator2(Key_Exchange_img2_l, Key_Exchange_img2_m)[0],label_0)\n",
    "        \n",
    "        loss_ce = loss_ce0 + loss_ce2\n",
    "        loss_CEs.append(loss_ce)\n",
    "        \n",
    "        loss_total = loss_R + 0.001 * loss_distance + 0.01 * loss_ce\n",
    "        loss_total.backward()\n",
    "        \n",
    "        #loss_Gs.append(loss_G)\n",
    "        \n",
    "        optimizer_E.step()\n",
    "        optimizer_DE.step()\n",
    "        \n",
    "        print(\"[Epoch %d/%d] [Batch %d/%d] [loss_R: %f][loss_D: %f][loss_Dis: %f][loss_CE: %f]\" % (epoch, 100, batches_done % len(train_loader), len(train_loader),loss_R.item(), loss_D.item(),loss_distance.item(),loss_ce.item()))\n",
    "        batches_done += 1\n",
    "        \n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        torch.save(encoder, 'KECAE_encoder_epoch_' + str(batches_done) + '.pth')\n",
    "        torch.save(decoder,'KECAE_decoder_epoch_' + str(batches_done) + '.pth')\n",
    "        torch.save(discriminator2,'KECAE_siamese_epoch_' + str(batches_done) + '.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
